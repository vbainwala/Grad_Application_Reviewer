{
  "document_name": "Bunn, Alec (110030446).pdf",
  "total_pages": 7,
  "pages": [
    {
      "page_number": 1,
      "content": "Alec Bunn\nabunn2@uw.edu | 509-279-4044\nEDUCATION\nBachelor of Science in Computer Science\nBachelor of Science in Mathematics\nUniversity of Washington, Seattle, WA\nExpected Graduation: May 2025\n Cumulative GPA: 3.76\n Honors: University of Washington Honor Society, Dean’s List\nRelated Coursework:\nMachine Learning, Deep Learning, Natural Language Processing, Autonomous Robotics,\nArtificial Intelligence, Algorithms, Hardware-Software Interface, Data Structures and\nParallelism, Data Visualization, Theory of Computations, Software Design & Implementation\nRESEARCH EXPERIENCE\nResearch Project on Supportive Data for In-Context Learning\nDr. Hannah Hajishirzi’s H2 Lab, University of Washington (January 2024 – Current)\n Collaborating with Dr. Ben Bogin and Dr. Sarah Wiegreffe to investigate training data\nproperties that enable large language models (LLMs) to learn in-context using data\nattribution techniques such as TRAK (Tracing with the Randomly-projected After Kernel)\nMachine Learning Research Internship\nMusic Information Retrieval Lab, Harvey Mudd College (Summer 2023-Spring 2024)\n Conducted research on music and audio processing with Dr. Timothy J. Tsai, Associate\nProfessor of Engineering and peers. Joint first author on a paper published in\nTransactions of the International Society for Music Information Retrieval (TISMIR).\n Paper citation: Jain, A., Bunn, A., Pham, A., & Tsai, T. (2024). PBSCR: The Piano Bootleg\nScore Composer Recognition Dataset. arXiv. https://arxiv.org/abs/2401.16803\n Presented findings at an international conference on the intersection of music and\nartificial intelligence at International Audio Laboratories in Erlangen, Germany. Our\ngroup then collaborated with Dr. Meinard Muller, and his PhD students on the findings.",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 2,
      "content": "I2 (Interactive Intelligence) Club\nUniversity of Washington (January 2022 - Present)\n Active member of a project-driven community focused on reinforcement learning,\nembodied AI, and neuroscience.\n Collaborated with different members on multiple AI research projects. One project\nexplored the possibility of getting two independent models to communicate with each\nother using their own de novo language. Another involved developing a model to meta-\nlearn a novel learning algorithm.\n Poster presentation of the results of first project at UW Research Symposium, Spring,\n2022. Presentation was titled, “Emergent Language: Independent AI Development of a\nLanguage-like Syntax.”\nPROFESSIONAL EXPERIENCE\nTeaching Assistant for Mathematics Department\nUniversity of Washington Mathematics Department (Fall 2024-Current)\n Instruct and help students in Ordinary Differential Equations Class. Assist professor as\nneeded.\nSoftware Engineering Internship\nEngage, Health Information Technology Company (Summer 2020)\n Developed a Python program employing machine learning techniques, primarily matrix\nmath applied to multivariate regression, to forecast hospital wait times.\n Link to actual hospital wait times my algorithm predicts:\nhttps://waittimes.inhs.org/Home?orgld=d175f125-360d-4321-a7b4-cbe0c27a706b_\nAdvanced Robotics at the University of Washington (ARUW)\n(October 2021 - January 2022)\n Engaged in robot design, CAD modeling (SolidWorks), manufacturing, and 3D printing.\nPROJECTS\nIn-Context Learning\n(Current, H2 lab)",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 3,
      "content": " Currently, investigating training data properties that enable large language models\n(LLMs) to learn in-context using data attribution techniques such as TRAK (Tracing with\nthe Randomly-projected After Kernel).\n Identifying key characteristics in samples that are important to the development of ICL\nthat could be illuminating for its mechanism. For instance, soft-copying (corresponding\nto the induction heads paper), parallel structure, burstiness, etc.\n Performing pretraining on models without this data to identify how important these\nsamples are.\nEmergent Language\n(I2 Club Project)\n Co-designed and coded (Python) an AI system that develops a de novo symbolic\nlanguage to communicate about a cooperative task. This project explores AI's potential\nfor developing linguistic skills that are grounded to objects in an environment.\nNeural Graphs\n(2023)\n Invented and implemented Neural Graphs, a machine learning approach using Graph\nNeural Networks (GNNs) to implement a meta-learning algorithm. This approach can\nacquire and optimize novel learning algorithms, including well-known techniques such\nas backpropagation.\nMusic and Audio Processing Projects\n(Summer 2023)\n Researched and completed two AI projects during Harvey Mudd College summer\ninternship. The first project was creating a new dataset for a composer classification\ntask. Various encoding schemes and architectures were tested, and their performances\ncompared. These included: transformers with various levels of pretraining, vision\ntransformers, CNNs, and RNNs.\n The second project involved cross-verifying audio clips of the same event to prevent\ntampering. We created a novel algorithm to do this based around the Hidden State\nTime Warping (HSTW) algorithm.\nReinforcement Learning Neural Graphs\n(2023)\n A partner and I applied Neural Graphs project to a Reinforcement Learning (RL) context.\nSpecifically, the Multi-Armed Bandit problem. We implemented a Neural Graph that\nlearns a novel RL algorithm for this problem for a specific time horizon. It beat all",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 4,
      "content": "baselines for a given time horizon including epsilon greedy, UCB-1 and UCB-2. Was\ncompleted for final project in Deep Learning class.\nLarge Language Model Chess Project\n(2023)\n A partner and I set out to determine the extent to which a LLM creates a “world model”\nwhen trained autoregressively. We trained an LLM autoregressively on PGN chess\nnotation strings. We then probed the model to see if we could reconstruct the chess\nboard given the model state. We found that we could not, which offers evidence that\nLLMs don’t implicitly construct a “world model”. This was completed for a final project\nin a Natural Language Processing course.\nSkills\n Programming Languages: Python, Java\n Frameworks/Tools: TensorFlow, PyTorch\n Technical Skills: Data analysis, machine learning algorithms, CAD modeling, audio\nanalysis\n Mathematics: Experience in problem solving, proofs, high-level mathematics areas such\nas linear algebra, linear optimization, real and complex analysis, topology, abstract\nalgebra\n Teaching: Experience tutoring and teaching mathematics on college level\nResearch Interests\nCurrently focused on understanding and improving machine reasoning capabilities and\nexploring how these insights can enhance individual lives and contribute to the greater good of\nhumanity.",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 5,
      "content": "Statement of Purpose\nAlec Bunn\nIn my exploration of the complexities of artificial intelligence, I am driven by fundamental\nquestions: Can we understand how machines reason? If so, can we enhance their learning capabilities to\nmake this process more efficient? If machines can fully develop these abilities, how can we best leverage\nthese models for the benefit of humanity? Inspired by these inquiries, I have actively pursued research\nfrom multiple perspectives. I believe that meta-learning holds significant promise for developing more\nefficient learners, a concept I have explored through various course projects particularly in the context of\nReinforcement Learning. I have a strong desire to continue to contribute to this exciting frontier of Artificial\nIntelligence (AI), where I aim to develop systems that can reason and learn with greater accuracy and\nefficiency. It is with this goal in mind that I am pursuing a PhD.\nResearch Projects. My curiosity and inquiry with Artificial Intelligence was fueled early in my\ncollegiate career when I joined Interactive Intelligence (I2), University of Washington’s AI club. Soon\nafter, a fellow club member and I began meeting and developing questions and ideas we wanted to\nexplore together. Some of these questions revolved around our desire to have two independent models\ncommunicate with their own de novo language to solve a cooperative task. Despite challenges and\nsetbacks, we persisted and succeeded in producing a focused example where two models successfully\ncommunicated with each other in a new language. We were then able to present this work in a poster\npresentation at the UW Research Symposium and from that we got ideas on ways to build on our work.\nMore recently, we have been exploring meta-learning and are working on a project to develop a model to\nmeta-learn a novel learning algorithm. This pursuit of efficient data learning remains a central theme to\nme, and one I hope to continue to pursue further in graduate school.\nIn the summer of 2023, I had the privilege of joining the Music Information Retrieval (MIR) Lab\nat Harvey Mudd College. As a research intern under Dr. Timothy Tsai, I joined a research team to study\nthe impact of AI in the field of music. During the summer, I made significant contributions on projects\ninvolving music and audio processing. One of the projects focused on predicting the composer of a piece",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 6,
      "content": "of music based on its compositional style. Working with a team of 3 other students, we constructed a\ndataset of classical music data and trained CNN, RNN, and Transformer-based models using modern\npretraining and fine-tuning practices. The project continued after the summer and well into the school\nyear with testing new ideas and weekly virtual meetings with Professor Tsai to discuss and refine our\nwork. We ended up producing a dataset that can be used for other music-related tasks such as training\nmodels for music classification or generation. I was a joint first author on the subsequent article that we\npublished in Transactions of the International Society for Music Information Retrieval (TISMIR). Dr. Tsai\nand his lab will continue to build on the work we accomplished. Near the end of the summer our research\nteam visited and presented our research and results at the International Audio Laboratories in Erlangen,\nGermany, where we collaborated with Dr. Meinard Müller and his team of PhD students. It was a great\nexperience to be able to present at the conference and collaborate with other researchers. Furthermore,\nthis experience helped me realize my absolute passion for research. I loved working and collaborating\nwith a team and grinding through the challenges of a problem and coming up with creative solutions.\nThis experience solidified my desire to pursue further research goals and be involved in an academic\nenvironment.\nLast year I began a research project with Dr. Ben Bogin and Dr. Sarah Wiegreffe, part of Dr.\nHanna Hajishirzi’s H2Lab, at the University of Washington, investigating the aspects of training data that\nallow large language models (LLMs) to perform In-Context Learning (ICL). I am still working on the\nproject and my work primarily involves using data attribution techniques, such as Tracing with the\nRandomly projected After Kernel (TRAK), to explore how specific data characteristics influence the\nacquisition of ICL. A few ideas for the mechanism of ICL have been proposed, however, the specific\naspects of the data that cause ICL to develop in a model are not as well studied. In other words, the\n“how” is well understood but less so the “why” which is what intrigues me. I am interested in the specifics\nof ICL which is essentially another meta-learned algorithm, and I want to analyze what sort of algorithm it\nimplements and if that idea can be generalized.\nFuture Work. All of my research opportunities as well as countless personally inspired projects\nhave led me to the conclusion that graduate school is for me. I am an innately curious person and have",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 7,
      "content": "spent many hours trying to solve problems that I have encountered both on my own and through\ncollaborating with friends and colleagues. I have a hunger to dive deeper into the field of AI and discover\nwhat exactly makes a machine able to reason and how these ideas and concepts can be applied to\nimprove our individual lives as well as humanity as a whole. I am excited about the possibility of\ncontinuing my studies at Columbia. The work of Professor Daniel Hsu in the area of statistical machine\nlearning and Professor Christos Papadimitriou on the mathematical foundations of machine learning align\nclosely with my interests in machine reasoning. Additionally, the research environment at Columbia, with\nits strong emphasis on interdisciplinary collaboration, is the ideal setting for me to pursue my research\ngoals. I am eager to embark on this journey into graduate study, where I can contribute to the\nadvancement of machine reasoning and meta-learning.",
      "metadata": {
        "width": 612,
        "height": 792
      }
    }
  ]
}