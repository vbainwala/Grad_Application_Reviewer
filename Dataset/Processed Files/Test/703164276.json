{
  "document_name": "Dugue, Zack (703164276).pdf",
  "total_pages": 2,
  "pages": [
    {
      "page_number": 1,
      "content": "Zack Dugue\n(cid:18) # zdugue@caltech.edu — (cid:132) (605) 877-0228 — § github.com/Zack-Dugue\nEDUCATION\nCalifornia Institute of Technology (Caltech) Expected: Jun. 2025\nBachelors of Science, Major in Computer Science, Minor in Mathematics. Overall GPA: 3.9/4.3\nRelevant Coursework:\nComputer Science: TheoryofComputation,Algorithms,MachineLearning,ComputingSystems,ComputerProgramming\nMath: Probability,BayesianStatistics,Statistics,DiscreteMath,AbstractAlgebra,DifferentialEquations,Multivariable\nCalculus\nMiscellaneous: Learning in Games, Game Theory, Quantum Mechanics\nRESEARCH EXPERIENCE\nTargeted Forgetting in Neural Networks - w/ Prof. Alexandra Brintrup (University of Cambridge) Jun. 2024 -\nPresent\n• Focusesonthetaskof”MachineUnlearning.”ThisisthetaskoftakingapretrainedNeuralNetworkandremoving\nthe influence of certain data points from that pre training dataset, without compromising performance on the\nremaining data.\n• Developedanovelalgorithmwhichtreatstheparametersoftheneuralnetworkasindependentgaussians,andthen\nuses the KL divergence between the parameters original model and the model we’re training to forget in order to\nprotect performance.\n• Research is ongoing currently, but me and my mentor Jack Foster are hoping to publish our findings in DMLR.\nBayesian Neural Networks for Continual Learning - w/ Prof. Antonio Rangel (Caltech) Jan. 2024 - Jun.\n• The goal of this research is to investigate certain hyperparameter schemes within the Bayesian Gradient Descent\nAlgorithm (BGD). BGD is an algorithm designed for training Bayesian Neural Networks for Continual Learning.\n• Derived a differential equation representation of the Bayesian Gradient Descent Algorithm in the limit of infinitely\nsmall optimization steps.\n• Wrote an implementation of the Bayesian Gradient Descent Algorithm in Pytorch, including a wrapper that takes\nin any pytorch neural network module and converts it into a Bayesian Neural Network.\n• Tested and Evaluated alternate hyperparameter configurations of the BGD Algorithm on the CIFAR10 Dataset.\nDeepNeuralNetworkArchitectureOptimization-SURFw/Prof. GeorgiaGkioxari(Caltech)Apr. -Sep. 2023\n• Designedandimplementedanattention-basedalternativetoskipconnectionsintransformerneuralnetworks. Specif-\nically,thearchitectureallowed”Blocks”(collectionsoflayers)toattendbacktotheoutputsofpriorlayersthrough\na learned query. The goal of this was to make a more expressive implementation of the na¨ıve “sum everything up”\naggregation found in skip-connection-based architectures.\n• ItestedthisimplementationonbothVision(CIFAR10)andNLP(Wikitext108)datasetsusingatrainingpipeline\nwritten with the pytorch library.\nNeural ODEs for Streaming Perception - SURF w/ Prof. Yisong Yue’s lab (Caltech) Jun. - Sep. 2022\n• Designed a custom data loader to sample sequences from the ArgoverseHD Object detection dataset.\n• Implemented a Neural ODE based feature backbone for a Fast-RCNN model. Unlike the standard implementation\nfor the CNN backbone, this Neural ODE backbone took in a sequence of images and processed them recurrently,\nallowing for a lighter-weight model that could reuse redundant features from prior frames. Code was implemented\nusing the pytorch library.\n• I used performance profiling to evaluate how different ODE solvers compared in speed, (and how the ODE model\nitself compared against the baseline model).\nSpiking Neural Network Research - SURF w/ Prof. Matt Thomson’s lab (Caltech) Jun. - Aug. 2021\n• Studiedthepropertiesof2DSpikingNeuralNetworkstrainedwithunsupervisedHebbianLearningforvariousvision\ntasks.\n• Created metrics and visualizations to understand the properties of 2D Spiking Neural Networks during training.\n• Programmed extensively in Matlab.\nExperimental Design for Study of Lunar Regolith - w/ Dr. Osazonamen Igbinosun (JPL) Jun. - Sep. 2020\n• UsingCADrefinedanexperimentaldesignforstudyingthepropertiesofthethermalconductivityofLunarRegolith\nunder varying user-defined pressures.\n• Collaborated with a fellow peer to study the heat transfer properties of this experimental design.",
      "metadata": {
        "width": 595.27,
        "height": 841.89
      }
    },
    {
      "page_number": 2,
      "content": "PROJECTS\nImplemented a Pipeline for Training NLP Models on Augmented Training Objectives Jul. - Aug. 2023\n• Adapted many Self Supervised Learning Algorithms used in Computer Vision to text based data (WikiText 103\nspecifically). This includes an implementation of BarlowTwins, Vector Quantized-VAEs, and an EMA encoder\nimplementation inspired by Dino.\n• Theprojectwasmotivatedmostlyoutofcuriosity. Thefinalresultwasthatmostofthesemethodsdidnottransfer\nwell at all to the language domain.\n• The question of whether there is a more effective training objective than simply next token prediction for language\nmodels is still of interest to me.\nChest X-ray Diagnosis Machine Learning Project Apr. - Jun. 2023\n• Our CS156b course, taught by Professor Abu-Mostafa, required that I and a team of 3 other students design\nand implement a Vision Transformer as part of a competition between other students. We were training and\nbeing evaluated on the Chexpert dataset, which contains images of chest X-rays alongside labels corresponding to\nradiologist-identified medical abnormalities present in the X-ray.\n• OurfinalimplementationusedaDinoV2pre-trainedTransformer,withafeed-forwardnetworkfine-tunedontopof\nthe embedding. We used many augmentation strategies and also made efforts to collect chest X-Ray data outside\nof Chexpert that we could use to further train our model.\nReinforcement Learning Project Sep. - Dec. 2023\n• As part of my final project for the class ”Learning in Games”, we used Reinforcement Learning to perform trades\nas a simulated market-maker.\n• Results showed that the policy learned was similar to an algorithm used commonly by market makers in industry.\nCreated a Functional Video Game Apr. - Jun. 2022\n• Designed a game from the ground up in C as part of Caltech’s Introduction to Software Design course.\n• Ourgamewascalled”DinoDefenders”andutilizedaphysicsenginetosimulatethetrajectoriesofAsteroids,slugs,\npropelled rockets, and lasers. The goal of the game was to destroy the enemy asteroids before your planet was\ndestroyed. The game contained several levels, sprite art, and music.\nProgrammed an App to Track User’s CO2 Emissions Jun. - Aug. 2019\n• ThiswaspartofaprojectfortheMITOnlineScience,Technology,andEngineeringCommunity(MOSTEC)project\ncourse in Mobile App Development.\n• Our group designed an app that used the make and model of your car combined with calculating your distance\ntraveledusingyourphone’sGPScoordinatestocalculatetheexpectedCO2outputoftheuser(atleastwithrespect\nto their vehicle use).\nTECHNICAL SKILLS\nLanguages: C, Java, Python, MATLAB, Maple, R, Shell Scripts, Assembly\nPackages/Frameworks: Git, Pandas, NumPy, Anaconda, LaTex, SKLearn, PyTorch, Pytorch Lightning, Matplotlib\nDeveloper Tools: VS Code, PyCharm, IntelliJ IDEA, AWS, Anaconda, SLURM",
      "metadata": {
        "width": 595.27,
        "height": 841.89
      }
    }
  ]
}