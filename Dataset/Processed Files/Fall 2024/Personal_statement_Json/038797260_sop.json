{
  "document_name": "Luo, Yuhong (038797260).pdf",
  "total_pages": 4,
  "pages": [
    {
      "page_number": 1,
      "content": "Luo, Yuhong; DOB: 10/31/1996; ID: 038797260\nYuhongLuo yuhongluo@umass.edu\nStatement of Purpose\nMyresearchinterestisinensuringthereliabilityandtrustworthinessofAI,withaspecificfocus\nonfairnessandrobustnessofcomplexmachinelearning(ML)systems.\nBeforeIbegantheM.S.programatUMass,IworkedasanengineeratAirbnbonmarketingcontent\nrecommendations. Myexperienceaddressingpositionalbiasinrecommendersystemsdeepenedmy\nunderstandingofmodernML’slimitations. IlearnedthatMLcanexhibitharmfulalgorithmicbi-\nases, resulting in unfairness in critical domains (Obermeyer et al., 2019), fostering the spread of\nfake news within echo chambers in social networks (Cinelli et al., 2021), and making the system\nsusceptibletoadversarialattacks(Mehrabietal.,2021). SincemodernMLsystemshavebecome\nincreasinglycomplex,manyunderlyingissuescontinuetopersistdespiteindustryeffortstoaddress\nthesebiases(Benderetal.,2021). Myknowledgeofthesebiasesfuelsmyinterestinstudyingthe\nreliabilityandtrustworthinessofmodernMLsystems.\nDriventoexploreemergingtechnologiesforenhancingthefairnessandrobustnessofrecommender\nsystemsine-commerceandsocialnetworks,Ibecameparticularlyexcitedaboutgraphrepresenta-\ntion learning for its role in anomaly detection (Ma et al., 2023). While I still worked at Airbnb, I\ninitiatedresearchwithProf.PanLiofGeorgiaTech,focusingonenhancingtheexpressivenessand\nscalabilityoftemporalgraphlearning.\nIfoundthataprovablyexpressiveapproachtotemporalgraphlearningbyWangetal.(2021)suf-\nfers from its slow process in training and inference, severely limiting its scalability. To mitigate\nthisissue,Iconductedfine-grainedprofilingofthelearningproceduresformultipleapproachesto\ntemporalgraphlearning(Wangetal.,2021;Kumaretal.,2019;Rossietal.,2020),andidentified\na promising lead: many recent methods need to trace back in time and sample historical neigh-\nborsforlearninganoderepresentation. Unfortunately,theseoperationsareslow. Ibrainstormed,\nexplored, andeventually arrivedat adynamic-programming-inspiredGPU-executable algorithm,\nwhichsignificantlyoutperformsbaselinesintermsofbothpredictionperformanceandscalability.\nWepublishedtheresult(Luo&Li,2022)attheLearningonGraphs(LoG)conference,andreceived\nabestpaperaward. Fromthisexperience,Ihavedevelopedastrongpassionforstudyingopen-ended\nproblemsandimprovingcomplexMLsystems,andamcommittedtopursuingaresearchcareer.\nEager to explore more research opportunities in the area of reliable and trustworthy AI, I joined\ntheM.S.CSprogramatUMass. IwasexcitedtoworkwithProf.HuiGuanandherteam,focusing\nonimprovingtheaccuracyofpersonalizedfederatedlearningwhichpreservesclientprivacywhile\nachieving personalized predictions. I approached this problem as the adaptation of a foundation\nmodeltoachievepersonalizedclientmodels,butencounteredchallengesinprovidingequitableper-\nformanceamongclientswithdatadistributionsunderstatisticalheterogeneity(Lietal.,2020). This\nalsoledmetodiscovercurrentchallengesinensuringthefairnessandrobustnessofalldownstream\ntasksofthefoundationmodels(Bommasanietal.,2022),whosesolutionscanprovideinsightsfor\nensuringfairclientperformanceinpersonalizedfederatedlearning. Iwasdrawntowardexploring\npotentialimprovementsthatcouldenhancethefairnessandrobustnessoffoundationmodelsand\ntheiradaptation.\nInoticedthatthebackboneoffoundationmodelsconsistsoflearneddatarepresentationsthatare\nvaluableforvariousdownstreampredictiontasks. However,providerswhodistributethefounda-\ntion models or data representations may have limited control over their use in downstream mod-\nels(Zemeletal.,2013;Madrasetal.,2018). Thisrealizationallowedmetovaluetheimportance\nofensuringthefairnessoflearnedrepresentationsinawaythatcanensurefairnessforalldown-\nstreampredictivetasksthatusetherepresentations. Todelvedeeperintothissubject,Iembarked\non a project on fair representation learning in collaboration with Prof. Philip S. Thomas at\nUMassandDr. AustinHoagfromBerkeleyExistentialRiskInitiative.\nIdiscoveredaknowledgegapincurrentfairrepresentationlearningmethods: manypriormethods\nlackhigh-confidenceguaranteesthatthelearnedrepresentationswillremainfairforfutureunseen\ndataandunfamiliardownstreamtasks. Toaddressthisgap, Idevelopedaframeworkcapableof\nPage1\n10/07/2024 09:54 Personal Statement 1/4",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 2,
      "content": "Luo, Yuhong; DOB: 10/31/1996; ID: 038797260\nYuhongLuo yuhongluo@umass.edu\nofferinghigh-confidencefairnessguaranteesinrepresentationlearningandsupportedthis\nclaimwithboththeoreticalandempiricalanalysis. Thispaper(Luoetal.,2023)iscurrentlyinsub-\nmission. Iplantocontinuethislineofworktofurtherexplorehigh-confidenceguaranteesforthe\nfairnessandrobustnessofrepresentationlearningunderdistributionalshiftsandtheadaptation\noffoundationalmodels.\nWithmygrowingconfidenceandaffinityinexploringandbridgingknowledgegapstoadvancethe\nfrontierofknowledge,IamdedicatedtoenhancingthereliabilityandtrustworthinessofAIthrough\nresearch. Formynextstep,IaimtopursueaPhDtoenhancemyresearchexpertiseandfurther\nrefinemyresearchfocus. IlookforwardtojoiningresearchlabsatColumbiaandlearningabout\nAI trustworthiness from different perspectives. While I am still exploring various aspects of AI\ntrustworthiness,thereareacoupleofdirectionsthatstandouttomeasonesthatcouldmakefor\ninterestingprojects.\nFairness Guarantees Under Distributional Shifts and Causality. During my research on\nprovidinghigh-confidencefairnessguaranteesforrepresentationlearning,Ilearnedthatclassical\nstatistical tools may be inadequate in guaranteeing fairness in the presence of certain distribu-\ntionalshifts,suchascovariateshifts(Singhetal.,2021). Conductingastatisticaltestonaholdout\ntest set is insufficient to ensure the validity of the results when experimental conditions change.\nAdditionally, these distributional shifts significantly affect the robustness and generalizability of\nML models (D’Amour et al., 2020; Wang et al., 2023). Several studies have suggested that these\nproblemsstemfromtheabsenceofcausalformalisms(Pearl,2019;Kaddouretal.,2022),andhave\nproposedincorporatinggraphicalcausalitytodistinguishshift-invariantcausalfactorsfromspuri-\nouscorrelations,includingtaskslikecausaldisentanglementofrepresentations(Yangetal.,2021),\ninvariantfeaturelearning(Sunetal.,2021;Luetal.,2022),andcausaldomainadaptation(Singh\netal.,2021). Iamcuriousunderwhatcircumstances(e.g.,datadistributionsandavailability)can\ntheidentifiabilityofthefairnessconstraintsbeguaranteed,howtoconstructfairnessguaranteesus-\ningcausalformalisms,andhowgeneralizabletheyareunderdistributionalshifts.\nRobustAdaptationofFoundationModels. Today,manylarge-scaleMLsystemsaretransition-\ningtotrainingfoundationmodelsonextensivedatasetsandthenadaptingthesemodelsthrough\nfine-tuningtovariousdownstreamtasksacrossdiversedomains(e.g.,GPT-4(OpenAI,2023)). Some\nrecent work has shown that foundation models can improve robustness to distribution shifts due\ntotheirtrainingondiversedataencompassingawidedistribution(Radfordetal.,2021). However,\ntheirrobustnessislimited,particularlywhenfine-tuningthefoundationmodelforfew-shotlearn-\ning because it can distort the pre-trained feature extractor (Xie et al., 2021; Kumar et al., 2022).\nWortsmanetal.(2022)showedthatensemblingtheweightsofthezero-shotandfine-tunedmodels\ncanleadtobetteraccuracywhilemaintainingrobustness. Itisalsosuggestedthatthereexistsa\nmodelthatleveragesthefoundationmodel’srobustnessandenhancesaccuracythroughfine-tuning.\nDrawinginspirationfromthelotterytickethypothesis(Frankle&Carbin,2019), Iaminterested\nin gaining fundamental understanding of robust adaptation by exploring whether it is possible to\nidentify sub-networks within the foundation model that contribute to robustness, how the subnet-\nworkschangewithfine-tuning,andhowthesesub-networksmayvaryacrossdownstreamtasks.\nColumbia University’s Computer Science PhD program stands out to me for several reasons.\nFirst, the faculty’s expertise on trustworthy AI is exceptional. I am eager to work with Richard\nZemelonfairnessandrobustnessinmodernAIsystems;withCarlVondrickonadversarialro-\nbustnessforlarge-scalemodels(Maoetal.,2023a,b);andwithDavidBleiandEliasBareinboim,\nastheirexpertiseinBayesianmethodsandcausalinferenceformsacrucialfoundationforensuring\nfairnessandrobustnessinAI.Columbia’srichcultureforinterdisciplinarycollaborationisanother\naspectthatinvigoratesme. Iamalsointerestedincollaboratingwithexpertsinotherfields,such\nastheDecision,Risk,andOperations(DRO)ofthebusinessschool,statisticsanddatascience,as\nIlookforwardtoopportunitiesforanalyzingAItrustworthinessindiversedomains.\nInthefuture, IaimtoapplytheresearchexpertiseIdevelopduringmyPhDtoaddressresearch\nquestionsconcerningthefairness,robustnessandtrustworthinessofcutting-edgetechnologies. I\nalsoaspiretocollaboratewithindustryresearcherstobringtheseideasintofruition.\nPage2\n10/07/2024 09:54 Personal Statement 2/4",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 3,
      "content": "Luo, Yuhong; DOB: 10/31/1996; ID: 038797260\nYuhongLuo yuhongluo@umass.edu\nReferences\nYuhongLuoandPanLi.Neighborhood-awarescalabletemporalnetworkrepresentationlearning.\nInLearningonGraphs,2022.\nYuhong Luo, Austin Hoag, and Philip S. Thomas. Learning fair representations with high-\nconfidenceguarantees. arXivpreprintarXiv:2310.15358,2023.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\ndangersofstochasticparrots: Canlanguagemodelsbetoobig? InProceedingsofthe2021ACM\nConferenceonFairness,Accountability,andTransparency,pp.610–623,2021.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\nShyamalBuch,DallasCard,RodrigoCastellon,NiladriChatterji,AnnieChen,KathleenCreel,\nJared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Ste-\nfanoErmon,JohnEtchemendy,KawinEthayarajh,LiFei-Fei,ChelseaFinn,TrevorGale,Lau-\nrenGillespie,KaranGoel,NoahGoodman,ShelbyGrossman,NeelGuha,TatsunoriHashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas\nIcard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling,\nFereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kudi-\ntipudi,AnanyaKumar,FaisalLadhak,MinaLee,TonyLee,JureLeskovec,IsabelleLevent,Xi-\nang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani,\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben New-\nman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel\nOrr,IsabelPapadimitriou,JoonSungPark,ChrisPiech,EvaPortelance,ChristopherPotts,Aditi\nRaghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan,\nChristopherRe´,DorsaSadigh,ShioriSagawa,KeshavSanthanam,AndyShih,KrishnanSrini-\nvasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Trame`r, Rose E. Wang, William\nWang,BohanWu,JiajunWu,YuhuaiWu,SangMichaelXie,MichihiroYasunaga,JiaxuanYou,\nMateiZaharia,MichaelZhang,TianyiZhang,XikunZhang,YuhuiZhang,LuciaZheng,Kaitlyn\nZhou, and Percy Liang. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258,2022.\nMatteoCinelli,GianmarcoDeFrancisciMorales,AlessandroGaleazzi,WalterQuattrociocchi,and\nMicheleStarnini. Theechochambereffectonsocialmedia. ProceedingsoftheNationalAcademy\nofSciences,118(9):e2023301118,2021.\nAlexanderD’Amour,KatherineHeller,DanMoldovan,BenAdlam,BabakAlipanahi,AlexBeutel,\nChristina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdi-\nari,NeilHoulsby,ShaoboHou,GhassenJerfel,AlanKarthikesalingam,MarioLucic,YianMa,\nCoryMcLean,DianaMincu,AkinoriMitani,AndreaMontanari,ZacharyNado,VivekNatarajan,\nChristopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica\nSchrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymy-\nrov,XuezhiWang,KellieWebster,SteveYadlowsky,TaedongYun,XiaohuaZhai,andD.Sculley.\nUnderspecificationpresentschallengesforcredibilityinmodernmachinelearning.arXivpreprint\narXiv:2011.03395,2020.\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable\nneuralnetworks. InICLR,2019.\nJeanKaddour,AengusLynch,QiLiu,MattJ.Kusner,andRicardoSilva.Causalmachinelearning:\nAsurveyandopenproblems. arXivpreprintarXiv:2206.15475,2022.\nAnanyaKumar,AditiRaghunathan,RobbieJones,TengyuMa,andPercyLiang. Fine-tuningcan\ndistortpretrainedfeaturesandunderperformout-of-distribution. InICLR,2022.\nSrijanKumar,XikunZhang,andJureLeskovec. Predictingdynamicembeddingtrajectoryintem-\nporalinteractionnetworks. InKDD,2019.\nTianLi,AnitKumarSahu,AmeetTalwalkar,andVirginiaSmith.Federatedlearning: Challenges,\nmethods,andfuturedirections. IEEESignalProcessingMagazine,37(3):50–60,2020.\nPage3\n10/07/2024 09:54 Personal Statement 3/4",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 4,
      "content": "Luo, Yuhong; DOB: 10/31/1996; ID: 038797260\nYuhongLuo yuhongluo@umass.edu\nChaochao Lu, Yuhuai Wu, Jose´ Miguel Herna´ndez-Lobato, and Bernhard Scho¨lkopf. Invariant\ncausalrepresentationlearningforout-of-distributiongeneralization.InInternationalConference\nonLearningRepresentations,2022.\nXiaoxiaoMa,JiaWu,ShanXue,JianYang,ChuanZhou,QuanZ.Sheng,HuiXiong,andLeman\nAkoglu. Acomprehensivesurveyongraphanomalydetectionwithdeeplearning. IEEETrans-\nactionsonKnowledgeandDataEngineering,35(12):12012–12038,2023.\nDavid Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair\nandtransferablerepresentations.InProceedingsofthe35thInternationalConferenceonMachine\nLearning,volume80ofProceedingsofMachineLearningResearch,pp.3384–3393.PMLR,2018.\nChengzhiMao,ScottGeng,JunfengYang,XinWang,andCarlVondrick. Understandingzero-shot\nadversarialrobustnessforlarge-scalemodels. InICLR,2023a.\nChengzhiMao,LingyuZhang,AbhishekJoshi,JunfengYang,HaoWang,andCarlVondrick. Ro-\nbustperceptionthroughequivariance. InICML,2023b.\nNinarehMehrabi,MuhammadNaveed,FredMorstatter,andAramGalstyan. Exacerbatingalgo-\nrithmicbiasthroughfairnessattacks. InAAAI,2021.\nZiadObermeyer,BrianPowers,ChristineVogeli,andSendhilMullainathan. Dissectingracialbias\ninanalgorithmusedtomanagethehealthofpopulations.Science,366(6464):447–453,2019.doi:\n10.1126/science.aax2342.\nOpenAI. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.\nJudeaPearl. Theseventoolsofcausalinference,withreflectionsonmachinelearning. Commun.\nACM,62(3):54–60,2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. arXiv\npreprintarXiv:2103.00020,2021.\nEmanueleRossi,BenChamberlain,FabrizioFrasca,DavideEynard,FedericoMonti,andMichael\nBronstein. Temporalgraphnetworksfordeeplearningondynamicgraphs. InICML2020Work-\nshoponGRL,2020.\nHarvineetSingh,RinaSingh,VishwaliMhasawade,andRumiChunara. Fairnessviolationsand\nmitigation under covariate shift. In Proceedings of the 2021 ACM Conference on Fairness, Ac-\ncountability,andTransparency,2021.\nXinwei Sun, Botong Wu, Xiangyu Zheng, Chang Liu, Wei Chen, Tao Qin, and Tie-Yan Liu. Re-\ncovering latent causal factor for generalization to distributional shifts. In Advances in Neural\nInformationProcessingSystems,2021.\nJindongWang,CuilingLan,ChangLiu,YidongOuyang,TaoQin,WangLu,YiqiangChen,Wenjun\nZeng, and Philip S. Yu. Generalizing to unseen domains: A survey on domain generalization.\nIEEETransactionsonKnowledgeandDataEngineering,35(8):8052–8072,2023.\nYanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation\nlearningintemporalnetworksviacausalanonymouswalks. InICLR,2021.\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Hanna Hajishirzi, Ali Farhadi,\nHongseokNamkoong, andLudwigSchmidt. Robustfine-tuningofzero-shotmodels. InCVPR,\n2022.\nSangMichaelXie,TengyuMa,andPercyLiang. Composedfine-tuning: Freezingpre-trainedde-\nnoisingautoencodersforimprovedgeneralization. InICML,2021.\nMengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae:\nDisentangledrepresentationlearningvianeuralstructuralcausalmodels. In2021IEEE/CVF\nConferenceonComputerVisionandPatternRecognition(CVPR),pp.9588–9597,2021.\nRich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representa-\ntions. InICML,2013.\nPage4\n10/07/2024 09:54 Personal Statement 4/4",
      "metadata": {
        "width": 612,
        "height": 792
      }
    }
  ]
}