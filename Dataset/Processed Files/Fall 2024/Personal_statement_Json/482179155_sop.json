{
  "document_name": "Su, Huangyuan (482179155).pdf",
  "total_pages": 3,
  "pages": [
    {
      "page_number": 1,
      "content": "Su, Huangyuan; DOB: 07/11/2000; ID: 482179155\nHuangyuan Su Fall 2024\nPersonal Statement\nMy past research interest lies in the intersection of reinforcement learning (RL) with robotics,\ncomputer vision, language models, and gametheory, etc. What excitesme themost isto develop\nagentsor algorithmsthat arerobustand generalizable.\nApplied ReinforcementLearning andRobot Learning\nAs a MS in Machine Learning studentatCarnegieMellonUniversity, Ihave hadtheprivilege to\nwork under the supervision of Prof. Jeff Schneider and Prof. Ruslan Salakhutdinov. My first\nproject at CMU found that the anchor embeddings used by recent trajectory prediction\napproaches can parameterize distinct discrete modes representing high-level driving behaviors.\nWe propose to perform fully reactive closed-loop planning over these discrete latent modes,\nallowing us to tractably model the causal interactions between agents at each step. We validate\nour approach on a suite of dynamic merging scenarios, finding that our approach avoids the\nfrozen robot problemwhich ispervasive inconventional planners.Our approachoutperformsthe\nprevious art inCARLA on challengingdense trafficscenarioswhenevaluated atrealistic speeds.\nIn the next project [4], we use diffusion models to perform language-guided closed-loop\nplanning for autonomous driving. Counter to conventional wisdom, we show that excessive\nconditioning information can hinder downstream planning performance due to poor\ngeneralization and lack ofdiversity. Additionally,weshowthat gradient-basedoptimization(e.g.\nclassifierguidance)canactually perform worsethan sampling-basedoptimization(e.g. CEM/ES)\nfor guided diffusion sampling. To this end, we introduce a novel sampling-based guidance\nmethod DiffusionES which handles arbitrary non-differentiable black-box objectives without\nretraining. We use LLMs to map language instructionsto programswhich adaptivelycontrolthe\nbehavior of our planner through reward shaping. With language supervision, our approach can\nsynthesize highly complex behaviors (e.g. aggressive lane weaving) not present in the training\ndata. Finally, we show that our approach can be used to solve the hardest nuPlan scenarios\nthrough languagefeedbackfrom ahumanexpert.\nAfter submitting these two projects to ICRA 24 and CVPR 24, we started work on using video\ngames for studying human-AI or multi-agent collaborative decision making in disaster\nmanagement.\nWith Prof. Salakhutdinov, we have been working on solving robotic control problems under\ndomain randomization across tasks and robot morphologies (hand, dog, arm) by distilling\nclassical optimization algorithms(MJPC) into visuomotorpolicies.\nImproving Reinforcement LearningAlgorithms using TheoreticalTools\nIn my first research project [1] during my bachelor at Nanyang Technological University, we\ntackled the failure of standard Policy iteration (PI) relies on Bellman‚Äôs Principle of Optimality\nunder time-inconsistent (TIC) objectives, such as non-exponentially discounted reward\nfunctions. Specifically, we consider an infinite-horizon TIC RL setting and formally present an\nalternative type of optimality drawn from game theory: subgame perfect equilibrium, that\nattempts to resolve the aforementioned questions. Drawing on these observations, we propose\nbackward Q-learning, a new algorithm in the approximate PI family that targets SPE policy\nunder non-exponentiallydiscountedreward functions.\n10/07/2024 09:54 Personal Statement 1/3",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 2,
      "content": "Su, Huangyuan; DOB: 07/11/2000; ID: 482179155\nHuangyuan Su Fall 2024\nThen [2], I noticedthat the learnedrepresentationof aùëÑ-networkand itstarget should,intheory,\nsatisfy a favorable distinguishable representation property. Specifically, there exists an upper\nbound on the representation similarity of the value functions of two adjacent time steps in a\ntypical DRL setting. However, in experiments, the DRL agents may violate this property and\nobtain asuboptimalpolicy.Therefore,weproposeasimple yeteffectiveregularizercalled Policy\nEvaluation with Easy Regularization on Representation, to maintain the distinguishable\nrepresentationproperty.These twoworksarepublishedin TMLRandCVPR 23,respectively.\nFutureplan\nIn my previous research endeavors, a significant portion of my time was dedicated to refining\nmodel architectures and hyperparameters. This challenge is exacerbated in the realm of RL,\nwhere the optimization of distinct parameter sets for different scenarios within the same task is\ncommon practice. This intricate process poses a significant obstacle when attempting to deploy\nour agents in real-world settings. Issues such as data shifts and long-tail scenarios often lead to\nsystem failures. Given the current momentum in industry towards integrating AI workers to\ncomplement or substitute human roles, it becomes imperative for these AI entities to possess\nspecific qualities, including robustness, safety, and consistency. Additionally, an essential\nattribute is their capacity for reasoning, enabling them to produce decisions that are not only\nrefined but also explainable to humans. The ability to incorporate feedback for continuous\nimprovementiscrucial, as theconsequences of inadequateperformance canbe severe.\nWhile certain studies [5, 6] suggest that agents based onLLMsoutperformRLagentsor exhibit\nzero-shot generalization, it is evident that their generalization capabilities remain constrained.\nConsequently, I strongly advocate for the development of better architectures, frameworks, or\ntraining schemes. For the first year of my PhD, I plan to explore various avenues, including\nleveraging generative (pre-)training for robust 3D recognition, distilling knowledge from\npretrained LLMs to mitigate the intensive sample requirements for training autonomous agents,\nand addressingthepaucity oftheoretical analysisindeep learningmethods.\nLooking ahead, my career goal is to work as a professor, contributing to research that yields\ntrustworthy AIagentsor systems,while mentoringandguiding motivatedstudents.\nWhy ColumbiaUniversityandComputerSciencePh.D.?\nBased on my experiences, I've come to recognize that effective problem-solving and the\ndiscovery of new, compellingchallenges require guidancefromthe mosttalented, insightful, and\nexperienced researchers. Among them, I am particularly drawn to the distinguished faculty at\nColumbia University, including Junfeng Yang, Carl Vondrick, Zhou Yu, Richard Zemel, and\nTony Dear. Their groundbreaking contributions in vision, NLP, and robotics, occasionally\ninfluenced by RL, strongly resonate with my researchinterests.A thorough examinationof their\nresearch, coupled with insights gained from perusing their lab environments, has strengthened\nmy desire to collaborate with them. In enrolling in thisCS PhDprogram, myforemost goalis to\nundergo systematic and comprehensive research training, honing my skills in problem-solving,\nenriching my expertise in my specific area while fostering a broad understanding of the field.\nThe program's resources for students planning faculty careers is particularly attractive, as it\nalignsseamlessly withmy long-term aspirationsof contributingmeaningfullyto academia.\n10/07/2024 09:54 Personal Statement 2/3",
      "metadata": {
        "width": 612,
        "height": 792
      }
    },
    {
      "page_number": 3,
      "content": "Su, Huangyuan; DOB: 07/11/2000; ID: 482179155\nHuangyuan Su Fall 2024\nReferences:\n[1] Lesmana,NixieS., Huangyuan Su,andChi SengPun. \"Reinventing PolicyIterationunder\nTimeInconsistency.\"Transactions onMachineLearningResearch(2022).\n[2] Qiang He,Huangyuan Su,XinwenHou, YuLiu. ‚ÄúFrustratinglyEasy Regularizationon\nRepresentationCan Boost DeepReinforcementLearning‚Äù. 2023IEEEComputer Society\nConferenceonComputer VisionandPattern Recognition.\n[3] BrianYang, Huangyuan Su,Nikolaos Gkanatsios,Tsung-Wei Ke,AyushJain,Jeff\nSchneider,KaterinaFragkiadaki. ‚ÄúControllable diffusion-basedplanning for language-guided\ndriving‚Äù. 2024In submission.\n[4] Adam Villaflor,Brian Yang,Huangyuan Su,Katerina Fragkiadaki,JohnM. Dolan,Jeff\nSchneider.‚ÄúTractableJoint Predictionand Planningover Discrete BehaviorModes for Urban\nDriving‚Äù.2024In submission.\n[5] Wang,Guanzhi,etal.\"Voyager:An open-endedembodiedagent withlargelanguage\nmodels.\"arXivpreprint arXiv:2305.16291 (2023).\n[6] Wu, Yue,etal.\"SPRING:GPT-4 Out-performs RLAlgorithms byStudyingPapersand\nReasoning.\"arXivpreprintarXiv:2305.15486(2023).\n10/07/2024 09:54 Personal Statement 3/3",
      "metadata": {
        "width": 612,
        "height": 792
      }
    }
  ]
}