{
    "ID": "038797260",
    "SOP": "YuhongLuo yuhongluo@umass.edu Statement of Purpose MyresearchinterestisinensuringthereliabilityandtrustworthinessofAI,withaspecificfocus onfairnessandrobustnessofcomplexmachinelearning(ML)systems. BeforeIbegantheM.S.programatUMass,IworkedasanengineeratAirbnbonmarketingcontent recommendations. Myexperienceaddressingpositionalbiasinrecommendersystemsdeepenedmy understandingofmodernML\u2019slimitations. IlearnedthatMLcanexhibitharmfulalgorithmicbi- ases, resulting in unfairness in critical domains (Obermeyer et al., 2019), fostering the spread of fake news within echo chambers in social networks (Cinelli et al., 2021), and making the system susceptibletoadversarialattacks(Mehrabietal.,2021). SincemodernMLsystemshavebecome increasinglycomplex,manyunderlyingissuescontinuetopersistdespiteindustryeffortstoaddress thesebiases(Benderetal.,2021). Myknowledgeofthesebiasesfuelsmyinterestinstudyingthe reliabilityandtrustworthinessofmodernMLsystems. Driventoexploreemergingtechnologiesforenhancingthefairnessandrobustnessofrecommender systemsine-commerceandsocialnetworks,Ibecameparticularlyexcitedaboutgraphrepresenta- tion learning for its role in anomaly detection (Ma et al., 2023). While I still worked at Airbnb, I initiatedresearchwithProf.PanLiofGeorgiaTech,focusingonenhancingtheexpressivenessand scalabilityoftemporalgraphlearning. IfoundthataprovablyexpressiveapproachtotemporalgraphlearningbyWangetal.(2021)suf- fers from its slow process in training and inference, severely limiting its scalability. To mitigate thisissue,Iconductedfine-grainedprofilingofthelearningproceduresformultipleapproachesto temporalgraphlearning(Wangetal.,2021;Kumaretal.,2019;Rossietal.,2020),andidentified a promising lead: many recent methods need to trace back in time and sample historical neigh- borsforlearninganoderepresentation. Unfortunately,theseoperationsareslow. Ibrainstormed, explored, andeventually arrivedat adynamic-programming-inspiredGPU-executable algorithm, whichsignificantlyoutperformsbaselinesintermsofbothpredictionperformanceandscalability. Wepublishedtheresult(Luo&Li,2022)attheLearningonGraphs(LoG)conference,andreceived abestpaperaward. Fromthisexperience,Ihavedevelopedastrongpassionforstudyingopen-ended problemsandimprovingcomplexMLsystems,andamcommittedtopursuingaresearchcareer. Eager to explore more research opportunities in the area of reliable and trustworthy AI, I joined theM.S.CSprogramatUMass. IwasexcitedtoworkwithProf.HuiGuanandherteam,focusing onimprovingtheaccuracyofpersonalizedfederatedlearningwhichpreservesclientprivacywhile achieving personalized predictions. I approached this problem as the adaptation of a foundation modeltoachievepersonalizedclientmodels,butencounteredchallengesinprovidingequitableper- formanceamongclientswithdatadistributionsunderstatisticalheterogeneity(Lietal.,2020). This alsoledmetodiscovercurrentchallengesinensuringthefairnessandrobustnessofalldownstream tasksofthefoundationmodels(Bommasanietal.,2022),whosesolutionscanprovideinsightsfor ensuringfairclientperformanceinpersonalizedfederatedlearning. Iwasdrawntowardexploring potentialimprovementsthatcouldenhancethefairnessandrobustnessoffoundationmodelsand theiradaptation. Inoticedthatthebackboneoffoundationmodelsconsistsoflearneddatarepresentationsthatare valuableforvariousdownstreampredictiontasks. However,providerswhodistributethefounda- tion models or data representations may have limited control over their use in downstream mod- els(Zemeletal.,2013;Madrasetal.,2018). Thisrealizationallowedmetovaluetheimportance ofensuringthefairnessoflearnedrepresentationsinawaythatcanensurefairnessforalldown- streampredictivetasksthatusetherepresentations. Todelvedeeperintothissubject,Iembarked on a project on fair representation learning in collaboration with Prof. Philip S. Thomas at UMassandDr. AustinHoagfromBerkeleyExistentialRiskInitiative. Idiscoveredaknowledgegapincurrentfairrepresentationlearningmethods: manypriormethods lackhigh-confidenceguaranteesthatthelearnedrepresentationswillremainfairforfutureunseen dataandunfamiliardownstreamtasks. Toaddressthisgap, Idevelopedaframeworkcapableof Page1 YuhongLuo yuhongluo@umass.edu offeringhigh-confidencefairnessguaranteesinrepresentationlearningandsupportedthis claimwithboththeoreticalandempiricalanalysis. Thispaper(Luoetal.,2023)iscurrentlyinsub- mission. Iplantocontinuethislineofworktofurtherexplorehigh-confidenceguaranteesforthe fairnessandrobustnessofrepresentationlearningunderdistributionalshiftsandtheadaptation offoundationalmodels. Withmygrowingconfidenceandaffinityinexploringandbridgingknowledgegapstoadvancethe frontierofknowledge,IamdedicatedtoenhancingthereliabilityandtrustworthinessofAIthrough research. Formynextstep,IaimtopursueaPhDtoenhancemyresearchexpertiseandfurther refinemyresearchfocus. IlookforwardtojoiningresearchlabsatColumbiaandlearningabout AI trustworthiness from different perspectives. While I am still exploring various aspects of AI trustworthiness,thereareacoupleofdirectionsthatstandouttomeasonesthatcouldmakefor interestingprojects. Fairness Guarantees Under Distributional Shifts and Causality. During my research on providinghigh-confidencefairnessguaranteesforrepresentationlearning,Ilearnedthatclassical statistical tools may be inadequate in guaranteeing fairness in the presence of certain distribu- tionalshifts,suchascovariateshifts(Singhetal.,2021). Conductingastatisticaltestonaholdout test set is insufficient to ensure the validity of the results when experimental conditions change. Additionally, these distributional shifts significantly affect the robustness and generalizability of ML models (D\u2019Amour et al., 2020; Wang et al., 2023). Several studies have suggested that these problemsstemfromtheabsenceofcausalformalisms(Pearl,2019;Kaddouretal.,2022),andhave proposedincorporatinggraphicalcausalitytodistinguishshift-invariantcausalfactorsfromspuri- ouscorrelations,includingtaskslikecausaldisentanglementofrepresentations(Yangetal.,2021), invariantfeaturelearning(Sunetal.,2021;Luetal.,2022),andcausaldomainadaptation(Singh etal.,2021). Iamcuriousunderwhatcircumstances(e.g.,datadistributionsandavailability)can theidentifiabilityofthefairnessconstraintsbeguaranteed,howtoconstructfairnessguaranteesus- ingcausalformalisms,andhowgeneralizabletheyareunderdistributionalshifts. RobustAdaptationofFoundationModels. Today,manylarge-scaleMLsystemsaretransition- ingtotrainingfoundationmodelsonextensivedatasetsandthenadaptingthesemodelsthrough fine-tuningtovariousdownstreamtasksacrossdiversedomains(e.g.,GPT-4(OpenAI,2023)). Some recent work has shown that foundation models can improve robustness to distribution shifts due totheirtrainingondiversedataencompassingawidedistribution(Radfordetal.,2021). However, theirrobustnessislimited,particularlywhenfine-tuningthefoundationmodelforfew-shotlearn- ing because it can distort the pre-trained feature extractor (Xie et al., 2021; Kumar et al., 2022). Wortsmanetal.(2022)showedthatensemblingtheweightsofthezero-shotandfine-tunedmodels canleadtobetteraccuracywhilemaintainingrobustness. Itisalsosuggestedthatthereexistsa modelthatleveragesthefoundationmodel\u2019srobustnessandenhancesaccuracythroughfine-tuning. Drawinginspirationfromthelotterytickethypothesis(Frankle&Carbin,2019), Iaminterested in gaining fundamental understanding of robust adaptation by exploring whether it is possible to identify sub-networks within the foundation model that contribute to robustness, how the subnet- workschangewithfine-tuning,andhowthesesub-networksmayvaryacrossdownstreamtasks. Columbia University\u2019s Computer Science PhD program stands out to me for several reasons. First, the faculty\u2019s expertise on trustworthy AI is exceptional. I am eager to work with Richard ZemelonfairnessandrobustnessinmodernAIsystems;withCarlVondrickonadversarialro- bustnessforlarge-scalemodels(Maoetal.,2023a,b);andwithDavidBleiandEliasBareinboim, astheirexpertiseinBayesianmethodsandcausalinferenceformsacrucialfoundationforensuring fairnessandrobustnessinAI.Columbia\u2019srichcultureforinterdisciplinarycollaborationisanother aspectthatinvigoratesme. Iamalsointerestedincollaboratingwithexpertsinotherfields,such astheDecision,Risk,andOperations(DRO)ofthebusinessschool,statisticsanddatascience,as IlookforwardtoopportunitiesforanalyzingAItrustworthinessindiversedomains. Inthefuture, IaimtoapplytheresearchexpertiseIdevelopduringmyPhDtoaddressresearch questionsconcerningthefairness,robustnessandtrustworthinessofcutting-edgetechnologies. I alsoaspiretocollaboratewithindustryresearcherstobringtheseideasintofruition. Page2 YuhongLuo yuhongluo@umass.edu References YuhongLuoandPanLi.Neighborhood-awarescalabletemporalnetworkrepresentationlearning. InLearningonGraphs,2022. Yuhong Luo, Austin Hoag, and Philip S. Thomas. Learning fair representations with high- confidenceguarantees. arXivpreprintarXiv:2310.15358,2023. Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangersofstochasticparrots: Canlanguagemodelsbetoobig? InProceedingsofthe2021ACM ConferenceonFairness,Accountability,andTransparency,pp.610\u2013623,2021. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, ShyamalBuch,DallasCard,RodrigoCastellon,NiladriChatterji,AnnieChen,KathleenCreel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Ste- fanoErmon,JohnEtchemendy,KawinEthayarajh,LiFei-Fei,ChelseaFinn,TrevorGale,Lau- renGillespie,KaranGoel,NoahGoodman,ShelbyGrossman,NeelGuha,TatsunoriHashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kudi- tipudi,AnanyaKumar,FaisalLadhak,MinaLee,TonyLee,JureLeskovec,IsabelleLevent,Xi- ang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben New- man, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr,IsabelPapadimitriou,JoonSungPark,ChrisPiech,EvaPortelance,ChristopherPotts,Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, ChristopherRe\u00b4,DorsaSadigh,ShioriSagawa,KeshavSanthanam,AndyShih,KrishnanSrini- vasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Trame`r, Rose E. Wang, William Wang,BohanWu,JiajunWu,YuhuaiWu,SangMichaelXie,MichihiroYasunaga,JiaxuanYou, MateiZaharia,MichaelZhang,TianyiZhang,XikunZhang,YuhuiZhang,LuciaZheng,Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258,2022. MatteoCinelli,GianmarcoDeFrancisciMorales,AlessandroGaleazzi,WalterQuattrociocchi,and MicheleStarnini. Theechochambereffectonsocialmedia. ProceedingsoftheNationalAcademy ofSciences,118(9):e2023301118,2021. AlexanderD\u2019Amour,KatherineHeller,DanMoldovan,BenAdlam,BabakAlipanahi,AlexBeutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdi- ari,NeilHoulsby,ShaoboHou,GhassenJerfel,AlanKarthikesalingam,MarioLucic,YianMa, CoryMcLean,DianaMincu,AkinoriMitani,AndreaMontanari,ZacharyNado,VivekNatarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymy- rov,XuezhiWang,KellieWebster,SteveYadlowsky,TaedongYun,XiaohuaZhai,andD.Sculley. Underspecificationpresentschallengesforcredibilityinmodernmachinelearning.arXivpreprint arXiv:2011.03395,2020. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neuralnetworks. InICLR,2019. JeanKaddour,AengusLynch,QiLiu,MattJ.Kusner,andRicardoSilva.Causalmachinelearning: Asurveyandopenproblems. arXivpreprintarXiv:2206.15475,2022. AnanyaKumar,AditiRaghunathan,RobbieJones,TengyuMa,andPercyLiang. Fine-tuningcan distortpretrainedfeaturesandunderperformout-of-distribution. InICLR,2022. SrijanKumar,XikunZhang,andJureLeskovec. Predictingdynamicembeddingtrajectoryintem- poralinteractionnetworks. InKDD,2019. TianLi,AnitKumarSahu,AmeetTalwalkar,andVirginiaSmith.Federatedlearning: Challenges, methods,andfuturedirections. IEEESignalProcessingMagazine,37(3):50\u201360,2020. Page3 YuhongLuo yuhongluo@umass.edu Chaochao Lu, Yuhuai Wu, Jose\u00b4 Miguel Herna\u00b4ndez-Lobato, and Bernhard Scho\u00a8lkopf. Invariant causalrepresentationlearningforout-of-distributiongeneralization.InInternationalConference onLearningRepresentations,2022. XiaoxiaoMa,JiaWu,ShanXue,JianYang,ChuanZhou,QuanZ.Sheng,HuiXiong,andLeman Akoglu. Acomprehensivesurveyongraphanomalydetectionwithdeeplearning. IEEETrans- actionsonKnowledgeandDataEngineering,35(12):12012\u201312038,2023. David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair andtransferablerepresentations.InProceedingsofthe35thInternationalConferenceonMachine Learning,volume80ofProceedingsofMachineLearningResearch,pp.3384\u20133393.PMLR,2018. ChengzhiMao,ScottGeng,JunfengYang,XinWang,andCarlVondrick. Understandingzero-shot adversarialrobustnessforlarge-scalemodels. InICLR,2023a. ChengzhiMao,LingyuZhang,AbhishekJoshi,JunfengYang,HaoWang,andCarlVondrick. Ro- bustperceptionthroughequivariance. InICML,2023b. NinarehMehrabi,MuhammadNaveed,FredMorstatter,andAramGalstyan. Exacerbatingalgo- rithmicbiasthroughfairnessattacks. InAAAI,2021. ZiadObermeyer,BrianPowers,ChristineVogeli,andSendhilMullainathan. Dissectingracialbias inanalgorithmusedtomanagethehealthofpopulations.Science,366(6464):447\u2013453,2019.doi: 10.1126/science.aax2342. OpenAI. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023. JudeaPearl. Theseventoolsofcausalinference,withreflectionsonmachinelearning. Commun. ACM,62(3):54\u201360,2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar- wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv preprintarXiv:2103.00020,2021. EmanueleRossi,BenChamberlain,FabrizioFrasca,DavideEynard,FedericoMonti,andMichael Bronstein. Temporalgraphnetworksfordeeplearningondynamicgraphs. InICML2020Work- shoponGRL,2020. HarvineetSingh,RinaSingh,VishwaliMhasawade,andRumiChunara. Fairnessviolationsand mitigation under covariate shift. In Proceedings of the 2021 ACM Conference on Fairness, Ac- countability,andTransparency,2021. Xinwei Sun, Botong Wu, Xiangyu Zheng, Chang Liu, Wei Chen, Tao Qin, and Tie-Yan Liu. Re- covering latent causal factor for generalization to distributional shifts. In Advances in Neural InformationProcessingSystems,2021. JindongWang,CuilingLan,ChangLiu,YidongOuyang,TaoQin,WangLu,YiqiangChen,Wenjun Zeng, and Philip S. Yu. Generalizing to unseen domains: A survey on domain generalization. IEEETransactionsonKnowledgeandDataEngineering,35(8):8052\u20138072,2023. Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation learningintemporalnetworksviacausalanonymouswalks. InICLR,2021. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Hanna Hajishirzi, Ali Farhadi, HongseokNamkoong, andLudwigSchmidt. Robustfine-tuningofzero-shotmodels. InCVPR, 2022. SangMichaelXie,TengyuMa,andPercyLiang. Composedfine-tuning: Freezingpre-trainedde- noisingautoencodersforimprovedgeneralization. InICML,2021. Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Disentangledrepresentationlearningvianeuralstructuralcausalmodels. In2021IEEE/CVF ConferenceonComputerVisionandPatternRecognition(CVPR),pp.9588\u20139597,2021. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representa- tions. InICML,2013. Page4",
    "Resume": "# jamesluoyh@gmail.com (cid:239) linkedin.com/in/yuhong-luo/ \u00a7 github.com/JamesLuoyh Education University of Massachusetts Amherst, MA, USA M.S. in Computer Science, 3.96 GPA September 2022 - December 2023 University of Washington Seattle, WA, USA B.S. double major in Computer Science and ACMS, 3.82 GPA September 2016 - March 2019 Publication \u2022 Yuhong Luo, Austin Hoag and Philip S. Thomas. Learning Fair Representations with High-Confidence Guarantees. In Submission. arXiv preprint arXiv:2310.15358, 2023. [pdf] \u2022 Yuhong Luo and Pan Li. Neighborhood-aware Scalable Temporal Network Representation Learning. In Learning on Graphs, 2022. Oral (4.6%), Best Paper Award. [pdf] Talk \u2022 Invited to present my work on Temporal Graph Learning at Stanford (in person), and at the Temporal Graph Reading Group (online) hosted jointly by McGill University, Mila, and University of Mannheim. [\u00af Recording ] Experience UMass Amherst, MA, USA Graduate Researcher with Prof. Philip Thomas on Fair Representation Learning (repo) February 2023 - September 2023 \u2022 Created a framework that offers high-confidence fairness guarantees for representation learning (paper submitted). Graduate Researcher with Prof. Hui Guan on Personalized Federated Learning (repo) September 2022 - December 2022 \u2022 Designed and implemented an algorithm for improving personalization in federated learning. Purdue & Georgia Tech Remote Partime Research Intern with Prof. Pan Li on Temporal Graph Learning (repo) May 2021 - August 2022 \u2022 Created a temporal graph neural network that improves SOTA expressiveness and efficiency (paper published). Airbnb San Francisco, CA, USA Software Engineer on Marketing Technology July 2019 - September 2022 \u2022 Built personalized marketing content recommendation models. \u2022 Designed and implemented marketing content building and campaign publishing tools. \u2022 Designed and implemented coupon management and issuance platform. UW Seattle, WA, USA Undergraduate Researcher with Prof. Jan Buys on Natural Language Processing (repo) January 2019 - March 2019 \u2022 Designed and implemented a graph learning algorithm to incorporate semantic graphs of sentences as input to LSTM. Teaching Assistant with Prof. Martin Tompa on Foundations of Computing II September 2018 - March 2019 \u2022 Held teaching sessions on topics including probability, statistics, randomized algorithms, introductory ML. Meta Seattle, WA, USA Software Engineer Intern July 2018 - September 2018 \u2022 Designed and implemented crawler for ads landing pages and algorithm for ads detection on landing pages. Yahoo San Francisco, CA, USA Software Engineer Intern July 2017 - September 2017 \u2022 Designed and implemented web services to power Sports App game alerts. Technical Skills Languages: Java, Python, C++, C, Ruby, SQL, Kotlin, php, JavaScript. Technologies: Deep Learning, NLP, Vision, Statistics, Graphical models, Reinforcement Learning. Frameworks: PyTorch, TensorFlow, React, Spark."
}