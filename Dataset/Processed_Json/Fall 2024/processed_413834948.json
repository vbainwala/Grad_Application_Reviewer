{
    "ID": "413834948",
    "SOP": "Wai Tak Lau (Michael) Personal Statement: Computer Science PhD at Columbia University ChatGPT told me that the determinant of a diagonal matrix is the sum of the diagonal elements, but it should be the product instead. 1Although trivial in this case, large language models hallucinating pose a serious problem when used in domains such as healthcare. One of the central problems in modern deep learning is that it fits to observational distribution and does not consider the cause and effect between different variables. These problems have motivated my research interest: 1) Learn causal representations of high-dimensional and multi-modal data, that allows models to perform well on unseen data and when few labels are available. These representations can be reused to build new models iteratively ; 2) Using the learnt causal representations for developing machine learning models in healthcare, applying to diseases classification, biomarker discovery and clinical deployment. I discovered my passion for research much later than my peers. When applying for masters program during senior year undergrad, my goal was to go into industry as I had yet to gain in depth knowledge and problems that I am passionate about. However, this changed as I dove deeper into different areas in machine learning and discovered many interesting and important problems. This led me to work in the AI4VS lab, led by Prof. Kaveri Thakoor, along with other amazing collaborators. Over the process, I grew to enjoy research and gained valuable experience. I have keen interest and am now looking to further develop my skills at Columbia University. 1 I asked ChatGPT to generate math questions and it failed to provide the correct answer. 1 Wai Tak Lau (Michael) Research at AI4VS Lab Data scarcity is one of the main problems in machine learning in healthcare, involving not only gathering a large amount of training data but also obtaining high- quality labeled data for training supervised algorithms. I am excited to address this problem by using data of different modalities, similar to how text and images can be combined. At AI4VS lab, we used eye tracking data to address this problem. Eye tracking has long been used to understand the subject's cognitive process when examining stimuli, and the relationship was proposed in the Eye-Mind hypothesis [1] [2]. Eye tracking data reflects the regions and order of importance as clinicians examine the reports, and they can be used as surrogate information that acts as labels in a situation where ground truth is not available. Most of the current approaches when using eye tracking data with images ignore the temporal aspect of the data, therefore the challenge is to create a reasonable encoding to capture both spatial and temporal relationships. To address this problem, I designed a new way to encode eye tracking information as words, inspired by natural language processing. A transformer encoder based model is then used to train with a multitask objective to learn meaningful embeddings where eye tracking on similar images are close to each other. Our goal is to use these embeddings to generate pseudo labels to train a model that uses optical coherence tomography (OCT) reports of the retina to predict Glaucoma (a neurodegenerative disease that leads to blindness). Although we need more data to support this conclusion, the results have shown that eye tracking data aids downstream tasks like Glaucoma classification. More surprisingly, eye tracking data alone achieves relatively high accuracy in classifying Glaucoma. During this process, I learnt how to maneuver in the complex search space of possible hypothesis and convert a hypothesis into concrete problems that we can solve. I presented this work at Columbia 2 Wai Tak Lau (Michael) Center of AI Technology Symposium: AI & Healthcare and I am working to submit this work to as the first author. In the context of healthcare, I am interested in developing and applying techniques in causal representation learning to learn interpretable representations that can aid understanding new biomarkers, interpretability and be used for downstream applications such as segmentation or even hypothesis generation. As a first step, we can work towards a joint embedding space between images and eye tracking, and even text from clinicians inspired by CLIP [4]. Other Projects and Experiences One interesting application that arises from GANs is performing image translation by generating samples from the conditional distribution P(x|y), where y is the input image and x is the translated image. However, this translation might not be accurate due to lack of conditioning on set of covariates z. For example z could be which city we would like to translate input into in Cityscape dataset [1], since different cities might have different architecture style. To overcome this challenge, we can not only generate from P(x|y,z) but also take into account the causal relationships, where each variable is modeled by a neural network. Figure 1: Example inputs and outputs from CityScape Dataset [1]. The dataset is collected from different cities. Left is the input, and right is output. 3 Wai Tak Lau (Michael) As I embark on my research journey, I have realized in many ways research is similar to entrepreneurship as they both require extreme ownership. During my undergraduate studies, I ventured into the entrepreneurial world through the Alchemy program at University of Illinois at Urbana-Champaign (UIUC), lead by Professor Sanjay Patel. Our goal was to build a minimum viable product for predicting realtime audio quality in the hopes that this information could be helpful for improving audio delivery and ultimately video as well. There were two main challenges: finding high quality training dataset that contains audio degradations and the real- time constraint. To solve the first problem, we decided to build a synthetic dataset by collecting our own data using Amazon Mechanical Turk. I helped design and build the survey as well as the different simulators to generate the synthetic audio dataset that consists 30 hours of audio. This gave me the chance to dive into audio quality metrics, compression codecs and audio quality degradations. To address the second problem, I also suggested the two stage features extractor and regression model to accommodate the real time needs. Ultimately, the model performed competitively and I learnt how to pivot around a obstacles creatively. At Columbia My long term goal is to become a researcher in using machine learning to enhance healthcare. I am especially excited to work with Professor Kaveri Thakoor and Professor Richard Zemel. I specifically want to work in the AI4VS Lab, as the lab and my interests aligns in robust and interpretable machine learning in healthcare using data from different sources and different modalities. Professor Richard Zemel\u2019s interests in few-shot learning and continual learning aligns with my interest learn useful representation that allows model to perform well when few labels are available and build models iteratively. I appreciate the close connection between the medical school and the computer science department, making it the ideal environment for me to transition from theory to application. With the excellent faculty, students 4 Wai Tak Lau (Michael) and collaborative environment, Columbia is the ideal place for me to continue my academic journey. References 1. Just, M. A., and Carpenter, P. A. (1976b). Eye fixations and cognitive processes. Psychol. Rev. 87, 329\u2013354. doi: 10.1016/0010-0285(76)90015-3 2. Rayner K. Eye movements in reading and information processing: 20 years of research. Psychol Bull. 1998 Nov;124(3):372-422. doi: 10.1037/0033-2909.124.3.372. PMID: 9849112. 3. M. Cordts, M. Omran, S. Ramos, et al., The cityscapes dataset for semantic urban scene understanding, 2016. DOI: 10.48550/ARXIV.1604.01685. [Online]. Available: https: // arxiv.org/abs/1604.01685. 4. Radford, A. et al. (2021) \u2018Learning Transferable Visual Models From Natural Language Supervision\u2019, arXiv [cs.CV]. Available at: http://arxiv.org/abs/2103.00020. 5",
    "Resume": "Michael Lau New York, NY \u2022 wl2822@columbia.edu \u2022 6303627274 \u2022 LinkedIn \u2022 Github SUMMARY Researcher skilled in machine learning, software development and data analysis with Python. Ready to contribute clean, efficient code to build systems. Strong collaborator, communicator, and initiative-driven to enhance research efforts. EDUCATION Columbia University New York, NY M.S. Electrical Engineering, Concentration: Machine Learning, GPA: 3.67/4.0 Feb 2023 Courses: Probabilistic Models and Machine Learning, Reinforcement Learning, Causal Inference 1 & 2, Natural Language Processing, Sparse Low-Dimensional Models, Statistical Learning, Deep Learning University of Illinois at Urbana-Champaign Champaign, IL B.S. (Hons) Computer Engineering, GPA: 3.58/4.0 May 2021 Honors: Dean\u2019s List (x2) Courses: Machine Learning, Algorithms, Deep Learning in Hardware, Image and Video Processing, Artificial Intelligence, Digital Signal Processing, Data Structure, Computer Security, Audio Computing Lab, Abstract Linear Algebra SKILLS \u2022 Python, C++, C, PyTorch, TensorFlow, R, SQL, GCP, AWS, JavaScript, HTML, MATLAB, CSS, Assembly, Git, CAD, Docker, scikit-learn, SciPy, Pandas WORK EXPERIENCE Columbia University March 2023 - Present Research Assistant - AI4VS Lab New York, NY \u2022 Developing and proposing self-supervised learning for ophthalmology where labels are scarce, utilizing eye tracking data and OCT reports for robust predictions and representation learning. \u2022 Leverage success in NLP to model eye tracking data based on BERT and Sentence-BERT to learn embeddings \u2022 Creating and conducting experiments, processing data to train deep learning models; implemented SimCLR and DINO from scratch with PyTorch and PyTorch Lightning. \u2022 Training, fine-tuning and validating Computer Vision models, leveraging domain knowledge from medical experts to guide model development, achieving 95% test-set accuracy in addition to improved interpretability LatentAI Jun 2022 - Aug 2022 Machine Learning Engineer Intern Princeton, NJ \u2022 Developed pruning methods on image recognition models using PyTorch, archived guaranteed 40% increase in inference speed on YOLOv5 Tiny. \u2022 Designed experiments to test hypothesis on different pruning strategies to increase inference speed on edge devices. \u2022 Programmed software tools in Python for standardized scalable deep learning experiments and; integrated pruning into LatentAI\u2019s end-to-end pipeline, benchmarking pruned models on target edge devices for the first time to assess inference time and accuracy. LiveSensus Jan 2020 - May 2021 Co-Founder Champaign, IL \u2022 Built a machine learning model and open-sourced dataset consisting of 30 hours of audio, labeled with MOS scores for quality estimation during Vo-IP. \u2022 Designed and developed simulators to re-create quality degradation in both videos and audios for dataset and survey launched on AWS and LiveSensus website. \u2022 Collaborated with four other founders, Professor Sanjay Patel and a leading live streaming company, five founders selected from 40 students under Alchemy Foundry at UIUC\u2019s Coordinated Science Laboratory. PROJECTS Vision Transformer for Glaucoma Classification using OCT Scans Dec 2022 \u2022 Proposed using Vision Transformer (ViT) for Glaucoma classification. Then use Latent Dirichlet Allocation (LDA) to interpret ViT\u2019s classification decisions on the neural network\u2019s attention weights across layers. \u2022 Achieved 95% test-set accuracy with ViT; used attention rollout to combine per OCT scan heatmap of important regions; LDA discover representations used by ViT for classification. Neural Causal Model for Image-to-Image Translation May 2022 \u2022 Designed and built a Neural Causal Model from scratch, consisting of deconvolution neural network and U-Nets trained as WGAN, for more robust conditional Image-to-Image translation. \u2022 Trained Neural Causal Model with Cityscape Dataset, where the G-constrained architecture takes labeled image and a set of covariates that guides the translation of the image."
}